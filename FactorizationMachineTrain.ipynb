{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "UxXcsdr3hCpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig8zOwT57FPo",
        "outputId": "fd90f158-b3fc-4935-813b-6d6468a1c0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset"
      ],
      "metadata": {
        "id": "hBwCOalmhBJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/CMPE256_project/train_final.csv')\n",
        "\n",
        "# Get the users, movies and ratings\n",
        "user_ids = train['User_ID'].values\n",
        "movie_ids = train['Movie_ID'].values\n",
        "ratings = train['Rating'].values\n",
        "\n",
        "# Count the number of users and the number of movies\n",
        "num_users = train['User_ID'].nunique()\n",
        "num_movies = train['Movie_ID'].nunique()"
      ],
      "metadata": {
        "id": "fV4xlXDF8OFq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-B4XE25mn8e",
        "outputId": "1aa014cb-2f5d-47af-f09b-640f64806b6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14583166, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sample(10000, random_state=42)\n",
        "\n",
        "# Get the users, movies and ratings\n",
        "user_ids = train['User_ID'].values\n",
        "movie_ids = train['Movie_ID'].values\n",
        "ratings = train['Rating'].values\n",
        "\n",
        "# Count the number of users and the number of movies\n",
        "num_users = train['User_ID'].nunique()\n",
        "num_movies = train['Movie_ID'].nunique()"
      ],
      "metadata": {
        "id": "gA2rOtjBH6IU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding Users and Movies\n",
        "### From ChatGPT Prompt:\n",
        "  \n",
        "* Can you make me a function in python that will convert my pandas data frame to one-hot encode my users and movies?\n",
        "\n",
        "\n",
        "This did not work as there are too many user/movie combinations. Needed to look for an alternative to one hot encoding all users and movies."
      ],
      "metadata": {
        "id": "mzTpzxJWgdl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def one_hot_encode_users_and_movies(df):\n",
        "  # Initialize OneHotEncoder for users and movies with sparse output as False to return a dense matrix\n",
        "  user_encoder = OneHotEncoder(sparse_output=False)\n",
        "  movie_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "  # Fit and transform User_ID and Movie_ID\n",
        "  user_encoded = user_encoder.fit_transform(df[['User_ID']])\n",
        "  movie_encoded = movie_encoder.fit_transform(df[['Movie_ID']])\n",
        "\n",
        "  # Convert encoded arrays to DataFrames\n",
        "  user_df = pd.DataFrame(user_encoded, columns=[f'User_{int(i)}' for i in range(user_encoded.shape[1])])\n",
        "  movie_df = pd.DataFrame(movie_encoded, columns=[f'Movie_{int(i)}' for i in range(movie_encoded.shape[1])])\n",
        "\n",
        "  # Concatenate the one-hot encoded columns with the original DataFrame\n",
        "  df_encoded = pd.concat([user_df, movie_df, df.drop(['User_ID', 'Movie_ID'], axis=1)], axis=1)\n",
        "\n",
        "  return df_encoded"
      ],
      "metadata": {
        "id": "qzt5G244gbzz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings for Users and Movies"
      ],
      "metadata": {
        "id": "v-IvAF0Hjf1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OjRJg4ABYN",
        "outputId": "d5730f7e-905a-4733-c927-0a75f3032e99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['User_ID', 'Rating', 'Movie_ID', 'Year', 'runtimeMinutes', 'movie',\n",
            "       'short', 'tvEpisode', 'tvMiniSeries', 'tvMovie', 'tvSeries', 'tvShort',\n",
            "       'tvSpecial', 'video', 'Action', 'Adult', 'Adventure', 'Animation',\n",
            "       'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family',\n",
            "       'Fantasy', 'Film-Noir', 'Game-Show', 'History', 'Horror', 'Music',\n",
            "       'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi',\n",
            "       'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Normalize user IDs and movie IDs\n",
        "user_ids = pd.Series(user_ids)  # Convert numpy array to pandas Series\n",
        "movie_ids = pd.Series(movie_ids)  # Convert numpy array to pandas Series\n",
        "user_id_mapping = {user_id: idx for idx, user_id in enumerate(user_ids.unique())}\n",
        "movie_id_mapping = {movie_id: idx for idx, movie_id in enumerate(movie_ids.unique())}\n",
        "\n",
        "# Re-index user and movie IDs\n",
        "user_ids_reindexed = user_ids.map(user_id_mapping)\n",
        "movie_ids_reindexed = movie_ids.map(movie_id_mapping)\n",
        "\n",
        "# Normalize numeric features\n",
        "numeric_features = train[['Year', 'runtimeMinutes']]\n",
        "scaler = StandardScaler()\n",
        "numeric_features_scaled = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# Binary features (already 0/1 in the dataset)\n",
        "binary_features = train[['Adult', 'movie', 'short', 'tvEpisode', 'tvMiniSeries',\n",
        "                         'tvMovie', 'tvSeries', 'tvSpecial', 'video', 'Action',\n",
        "                         'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
        "                         'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir',\n",
        "                         'History', 'Horror', 'Music', 'Musical', 'Mystery', 'News',\n",
        "                         'Reality-TV', 'Romance', 'Sci-Fi', 'Short', 'Sport',\n",
        "                         'Talk-Show', 'Thriller', 'War', 'Western']]\n",
        "\n",
        "# Concatenate scaled numeric and binary features\n",
        "other_features = pd.concat([pd.DataFrame(numeric_features_scaled, columns=['Year', 'runtimeMinutes']), binary_features], axis=1).fillna(0)\n",
        "other_features_input = tf.keras.layers.Input(shape=(other_features.shape[1],), name='OtherFeatures')\n",
        "\n",
        "# Normalize ratings\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "ratings_normalized = scaler.fit_transform(ratings.reshape(-1, 1))\n",
        "\n",
        "# Debugging Layer\n",
        "class DebugLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        tf.debugging.check_numerics(inputs, \"Found NaN or Inf in layer!\")\n",
        "        return inputs\n",
        "\n",
        "# Define the input layers for User_ID and Movie_ID\n",
        "user_input = tf.keras.layers.Input(shape=(1,), name='User')\n",
        "movie_input = tf.keras.layers.Input(shape=(1,), name='Movie')\n",
        "\n",
        "# Embedding layers with smaller dimensionality\n",
        "user_embedding = tf.keras.layers.Embedding(input_dim=len(user_id_mapping), output_dim=16, embeddings_initializer=GlorotNormal())(user_input)\n",
        "movie_embedding = tf.keras.layers.Embedding(input_dim=len(movie_id_mapping), output_dim=16, embeddings_initializer=GlorotNormal())(movie_input)\n",
        "\n",
        "# Flatten the embeddings and debug\n",
        "user_vec = DebugLayer()(tf.keras.layers.Flatten()(user_embedding))\n",
        "movie_vec = DebugLayer()(tf.keras.layers.Flatten()(movie_embedding))\n",
        "\n",
        "# Concatenate embeddings and other features\n",
        "concat = DebugLayer()(tf.keras.layers.Concatenate()([user_vec, movie_vec, other_features_input]))\n",
        "\n",
        "# Add dense layers with batch normalization and LeakyReLU activation\n",
        "x = tf.keras.layers.BatchNormalization()(concat)\n",
        "x = tf.keras.layers.Dense(64)(x)\n",
        "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(32)(x)\n",
        "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Output layer\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "# Define and compile the model\n",
        "model = tf.keras.models.Model(inputs=[user_input, movie_input, other_features_input], outputs=output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Debugging callback\n",
        "class CheckNaNCallback(tf.keras.callbacks.Callback):\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        if np.any(np.isnan(logs['loss'])):\n",
        "            print(f\"NaN detected at batch {batch}\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# Train the model\n",
        "model.fit([user_ids_reindexed, movie_ids_reindexed, other_features],\n",
        "          ratings_normalized,\n",
        "          batch_size=128,\n",
        "          epochs=20,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[CheckNaNCallback()])"
      ],
      "metadata": {
        "id": "VCoDXd2LjqmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a801426b-8270-49f8-8b45-bddccae6c188"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 1.0919 - root_mean_squared_error: 1.0430 - val_loss: 0.3681 - val_root_mean_squared_error: 0.6067\n",
            "Epoch 2/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3910 - root_mean_squared_error: 0.6243 - val_loss: 0.3070 - val_root_mean_squared_error: 0.5541\n",
            "Epoch 3/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1987 - root_mean_squared_error: 0.4456 - val_loss: 0.2709 - val_root_mean_squared_error: 0.5205\n",
            "Epoch 4/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1255 - root_mean_squared_error: 0.3542 - val_loss: 0.2511 - val_root_mean_squared_error: 0.5011\n",
            "Epoch 5/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0845 - root_mean_squared_error: 0.2907 - val_loss: 0.2359 - val_root_mean_squared_error: 0.4857\n",
            "Epoch 6/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0643 - root_mean_squared_error: 0.2534 - val_loss: 0.2088 - val_root_mean_squared_error: 0.4570\n",
            "Epoch 7/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0492 - root_mean_squared_error: 0.2219 - val_loss: 0.1766 - val_root_mean_squared_error: 0.4202\n",
            "Epoch 8/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0386 - root_mean_squared_error: 0.1964 - val_loss: 0.1484 - val_root_mean_squared_error: 0.3852\n",
            "Epoch 9/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0307 - root_mean_squared_error: 0.1753 - val_loss: 0.1366 - val_root_mean_squared_error: 0.3697\n",
            "Epoch 10/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0253 - root_mean_squared_error: 0.1591 - val_loss: 0.1373 - val_root_mean_squared_error: 0.3705\n",
            "Epoch 11/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0241 - root_mean_squared_error: 0.1553 - val_loss: 0.1422 - val_root_mean_squared_error: 0.3771\n",
            "Epoch 12/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0206 - root_mean_squared_error: 0.1436 - val_loss: 0.1471 - val_root_mean_squared_error: 0.3836\n",
            "Epoch 13/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0175 - root_mean_squared_error: 0.1322 - val_loss: 0.1503 - val_root_mean_squared_error: 0.3877\n",
            "Epoch 14/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0167 - root_mean_squared_error: 0.1290 - val_loss: 0.1521 - val_root_mean_squared_error: 0.3899\n",
            "Epoch 15/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0177 - root_mean_squared_error: 0.1330 - val_loss: 0.1514 - val_root_mean_squared_error: 0.3891\n",
            "Epoch 16/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0148 - root_mean_squared_error: 0.1217 - val_loss: 0.1506 - val_root_mean_squared_error: 0.3880\n",
            "Epoch 17/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - root_mean_squared_error: 0.1143 - val_loss: 0.1507 - val_root_mean_squared_error: 0.3882\n",
            "Epoch 18/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0128 - root_mean_squared_error: 0.1129 - val_loss: 0.1492 - val_root_mean_squared_error: 0.3862\n",
            "Epoch 19/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0131 - root_mean_squared_error: 0.1145 - val_loss: 0.1476 - val_root_mean_squared_error: 0.3841\n",
            "Epoch 20/20\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0124 - root_mean_squared_error: 0.1114 - val_loss: 0.1476 - val_root_mean_squared_error: 0.3842\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cd51de70a90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yF_ZEviMtCws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}